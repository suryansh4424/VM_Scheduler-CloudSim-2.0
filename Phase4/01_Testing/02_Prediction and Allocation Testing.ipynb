{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81cd3143",
   "metadata": {},
   "source": [
    "# Testing Prediction Accuracy and Error Ratio for CPU Usage with VM Allocation Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6516da40",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">CPU Usage Prediction and VM Allocation Simulation - </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "882db28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Model - MSE: 55.45, RMSE: 7.45, R²: 0.68\n",
      "Significant prediction error: Predicted 16.649491727565827%, Actual 4.0%, Difference 12.649491727565827%.\n",
      "Significant prediction error: Predicted 16.752317624233118%, Actual 2.0%, Difference 14.752317624233118%.\n",
      "Significant prediction error: Predicted 17.060068917402834%, Actual 2.0%, Difference 15.060068917402834%.\n",
      "Significant prediction error: Predicted 16.99174158174463%, Actual 6.0%, Difference 10.99174158174463%.\n",
      "Significant prediction error: Predicted 17.16361499632829%, Actual 0.0%, Difference 17.16361499632829%.\n",
      "Allocating 1 VM(s) due to high cumulative usage (84.6172348472747%).\n",
      "Significant prediction error: Predicted 16.896928129245335%, Actual 5.0%, Difference 11.896928129245335%.\n",
      "Significant prediction error: Predicted 17.114791284605236%, Actual 4.0%, Difference 13.114791284605236%.\n",
      "Significant prediction error: Predicted 16.80633342173532%, Actual 0.0%, Difference 16.80633342173532%.\n",
      "Significant prediction error: Predicted 16.691104283097307%, Actual 0.0%, Difference 16.691104283097307%.\n",
      "Significant prediction error: Predicted 16.561810528700974%, Actual 49.0%, Difference 32.438189471299026%.\n",
      "Allocating 1 VM(s) due to high cumulative usage (84.07096764738417%).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import simpy\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'C:/Users/ratho/OneDrive/Desktop/dataset1.txt'\n",
    "data = pd.read_csv(file_path, header=None)\n",
    "cpu_usage = data.values.flatten()\n",
    "\n",
    "sequence_length = 10\n",
    "\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        sequences.append(data[i:i + seq_length])\n",
    "        targets.append(data[i + seq_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "X, y = create_sequences(cpu_usage, sequence_length)\n",
    "X_flattened = X.reshape(-1, sequence_length)\n",
    "y_flattened = y\n",
    "\n",
    "# Scale the data\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X_flattened)\n",
    "y_scaled = scaler_y.fit_transform(y_flattened.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, train_size=0.7, shuffle=False)\n",
    "\n",
    "# Define and train the Linear Regression model\n",
    "model_lr = LinearRegression()\n",
    "model_lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "lr_predictions = model_lr.predict(X_test)\n",
    "\n",
    "# Rescale predictions\n",
    "lr_predictions_inv = scaler_y.inverse_transform(lr_predictions.reshape(-1, 1)).flatten()\n",
    "y_test_inv = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Evaluate the model\n",
    "mse_lr = mean_squared_error(y_test_inv, lr_predictions_inv)\n",
    "rmse_lr = np.sqrt(mse_lr)\n",
    "r2_lr = r2_score(y_test_inv, lr_predictions_inv)\n",
    "\n",
    "print(f'Linear Regression Model - MSE: {mse_lr:.2f}, RMSE: {rmse_lr:.2f}, R²: {r2_lr:.2f}')\n",
    "\n",
    "# SimPy Datacenter Simulation\n",
    "\n",
    "class Datacenter:\n",
    "    def __init__(self, env, capacity=10):\n",
    "        self.env = env\n",
    "        self.server = simpy.Resource(env, capacity=capacity)\n",
    "\n",
    "    def process_vm_request(self, vm, duration):\n",
    "        with self.server.request() as req:\n",
    "            yield self.env.timeout(duration)\n",
    "\n",
    "class VMAllocator:\n",
    "    def __init__(self, env, threshold=80):\n",
    "        self.env = env\n",
    "        self.threshold = threshold\n",
    "        self.cumulative_cpu_usage = 0\n",
    "        self.vm_counter = 0\n",
    "\n",
    "    def allocate_vms(self, predicted_value, actual_value):\n",
    "        self.cumulative_cpu_usage += predicted_value\n",
    "        difference = abs(predicted_value - actual_value)\n",
    "\n",
    "        if difference > 10:\n",
    "            print(f\"Significant prediction error: Predicted {predicted_value}%, Actual {actual_value}%, Difference {difference}%.\")\n",
    "\n",
    "\n",
    "        if self.cumulative_cpu_usage > self.threshold:\n",
    "            num_vms = int((self.cumulative_cpu_usage - self.threshold) // 10) + 1\n",
    "            print(f\"Allocating {num_vms} VM(s) due to high cumulative usage ({self.cumulative_cpu_usage}%).\")\n",
    "            self.cumulative_cpu_usage = 0 \n",
    "\n",
    "\n",
    "env = simpy.Environment()\n",
    "\n",
    "datacenter = Datacenter(env)\n",
    "vm_allocator = VMAllocator(env)\n",
    "\n",
    "\n",
    "last_n_values = X_scaled[-1]  \n",
    "predicted_values = []\n",
    "actual_values = y_test_inv[:10]  \n",
    "\n",
    "for i, actual_value in enumerate(actual_values):\n",
    "    predicted_value = scaler_y.inverse_transform(\n",
    "        model_lr.predict([last_n_values]).reshape(-1, 1)\n",
    "    ).flatten()[0]\n",
    "    predicted_values.append(predicted_value)\n",
    " \n",
    "    vm_allocator.allocate_vms(predicted_value, actual_value)\n",
    "\n",
    "    last_n_values = np.append(last_n_values[1:], scaler_y.transform([[predicted_value]])).flatten()\n",
    "\n",
    "    vm = f\"VM_{i}_Predicted_{predicted_value:.2f}\"\n",
    "    env.process(datacenter.process_vm_request(vm, 10))\n",
    "\n",
    "# Run the simulation\n",
    "env.run(until=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3074ccb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m940/940\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
      "Linear Regression Model - MSE: 55.37, RMSE: 7.44, R²: 0.68\n",
      "Significant prediction error: Predicted 2.869105100631714%, Actual 49.0%, Difference 46.130894899368286%.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense, Input\n",
    "import simpy\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'C:/Users/ratho/OneDrive/Desktop/dataset1.txt'\n",
    "data = pd.read_csv(file_path, header=None)\n",
    "cpu_usage = data.values.flatten()\n",
    "\n",
    "sequence_length = 10\n",
    "\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        sequences.append(data[i:i + seq_length])\n",
    "        targets.append(data[i + seq_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "X, y = create_sequences(cpu_usage, sequence_length)\n",
    "X_flattened = X.reshape(-1, sequence_length)\n",
    "y_flattened = y\n",
    "\n",
    "# Scale the data\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X_flattened)\n",
    "y_scaled = scaler_y.fit_transform(y_flattened.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, train_size=0.7, shuffle=False)\n",
    "\n",
    "# Define and train the GRU model\n",
    "gru_model = Sequential()\n",
    "gru_model.add(Input(shape=(sequence_length, 1)))  # Input layer\n",
    "gru_model.add(GRU(50, activation='relu'))\n",
    "gru_model.add(Dense(1))\n",
    "\n",
    "gru_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Reshape data for GRU input\n",
    "X_train_gru = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test_gru = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Train the GRU model\n",
    "gru_model.fit(X_train_gru, y_train, epochs=20, batch_size=32, verbose=0)\n",
    "\n",
    "# Predict using GRU\n",
    "gru_predictions = gru_model.predict(X_test_gru)\n",
    "\n",
    "# Apply Linear Regression to the GRU predictions\n",
    "model_lr = LinearRegression()\n",
    "model_lr.fit(gru_predictions, y_test)\n",
    "\n",
    "# Predict with Linear Regression on GRU predictions\n",
    "lr_predictions = model_lr.predict(gru_predictions)\n",
    "\n",
    "# Rescale predictions\n",
    "lr_predictions_inv = scaler_y.inverse_transform(lr_predictions.reshape(-1, 1)).flatten()\n",
    "y_test_inv = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Evaluate the model\n",
    "mse_lr = mean_squared_error(y_test_inv, lr_predictions_inv)\n",
    "rmse_lr = np.sqrt(mse_lr)\n",
    "r2_lr = r2_score(y_test_inv, lr_predictions_inv)\n",
    "\n",
    "print(f'Linear Regression Model - MSE: {mse_lr:.2f}, RMSE: {rmse_lr:.2f}, R²: {r2_lr:.2f}')\n",
    "\n",
    "# SimPy Datacenter Simulation\n",
    "\n",
    "class Datacenter:\n",
    "    def __init__(self, env, capacity=10):\n",
    "        self.env = env\n",
    "        self.server = simpy.Resource(env, capacity=capacity)\n",
    "\n",
    "    def process_vm_request(self, vm, duration):\n",
    "        with self.server.request() as req:\n",
    "            yield self.env.timeout(duration)\n",
    "\n",
    "class VMAllocator:\n",
    "    def __init__(self, env, threshold=80):\n",
    "        self.env = env\n",
    "        self.threshold = threshold\n",
    "        self.cumulative_cpu_usage = 0\n",
    "        self.vm_counter = 0\n",
    "\n",
    "    def allocate_vms(self, predicted_value, actual_value):\n",
    "        self.cumulative_cpu_usage += predicted_value\n",
    "        difference = abs(predicted_value - actual_value)\n",
    "\n",
    "        if difference > 10:\n",
    "            print(f\"Significant prediction error: Predicted {predicted_value}%, Actual {actual_value}%, Difference {difference}%.\")\n",
    "\n",
    "        if self.cumulative_cpu_usage > self.threshold:\n",
    "            num_vms = int((self.cumulative_cpu_usage - self.threshold) // 10) + 1\n",
    "            print(f\"Allocating {num_vms} VM(s) due to high cumulative usage ({self.cumulative_cpu_usage}%).\")\n",
    "            self.cumulative_cpu_usage = 0 \n",
    "\n",
    "# Create SimPy environment\n",
    "env = simpy.Environment()\n",
    "\n",
    "# Create Datacenter\n",
    "datacenter = Datacenter(env)\n",
    "\n",
    "# Create VM Allocator\n",
    "vm_allocator = VMAllocator(env)\n",
    "\n",
    "# Initialize the last_n_values from the training set\n",
    "last_n_values = X_scaled[-1]  # Get the last sequence from the dataset\n",
    "predicted_values = []\n",
    "actual_values = y_test_inv[:10]  # Get the first 10 test values\n",
    "\n",
    "for i, actual_value in enumerate(actual_values):\n",
    "    predicted_value = scaler_y.inverse_transform(\n",
    "        model_lr.predict([gru_predictions[i]]).reshape(-1, 1)\n",
    "    ).flatten()[0]\n",
    "    predicted_values.append(predicted_value)\n",
    " \n",
    "    # Allocate VMs based on the predicted value\n",
    "    vm_allocator.allocate_vms(predicted_value, actual_value)\n",
    "\n",
    "    last_n_values = np.append(last_n_values[1:], scaler_y.transform([[predicted_value]])).flatten()\n",
    "\n",
    "    vm = f\"VM_{i}_Predicted_{predicted_value:.2f}\"\n",
    "    env.process(datacenter.process_vm_request(vm, 10))\n",
    "\n",
    "# Run the simulation\n",
    "env.run(until=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53396ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ratho\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1097/1097\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 0.0106\n",
      "Epoch 2/10\n",
      "\u001b[1m1097/1097\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.0093\n",
      "Epoch 3/10\n",
      "\u001b[1m1097/1097\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.0094\n",
      "Epoch 4/10\n",
      "\u001b[1m1097/1097\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0095\n",
      "Epoch 5/10\n",
      "\u001b[1m1097/1097\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0095\n",
      "Epoch 6/10\n",
      "\u001b[1m1097/1097\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0094\n",
      "Epoch 7/10\n",
      "\u001b[1m1097/1097\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.0095\n",
      "Epoch 8/10\n",
      "\u001b[1m1097/1097\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.0092\n",
      "Epoch 9/10\n",
      "\u001b[1m1097/1097\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - loss: 0.0095\n",
      "Epoch 10/10\n",
      "\u001b[1m1097/1097\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0092\n",
      "\u001b[1m940/940\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step\n",
      "Bidirectional LSTM Model - MSE: 217.99, RMSE: 14.76, R²: -12466.31\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Significant prediction error: Predicted 16.650196075439453%, Actual 0.04040404040404041%, Difference 16.60979203503541%.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Significant prediction error: Predicted 16.77923583984375%, Actual 0.020202020202020204%, Difference 16.75903381964173%.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Significant prediction error: Predicted 17.060104370117188%, Actual 0.020202020202020204%, Difference 17.039902349915167%.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "Significant prediction error: Predicted 17.04277992248535%, Actual 0.06060606060606061%, Difference 16.98217386187929%.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Significant prediction error: Predicted 17.203746795654297%, Actual 0.0%, Difference 17.203746795654297%.\n",
      "Allocating 1 VM(s) due to high cumulative usage (84.73606300354004%).\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Significant prediction error: Predicted 16.970531463623047%, Actual 0.05050505050505051%, Difference 16.920026413117995%.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "Significant prediction error: Predicted 17.25579071044922%, Actual 0.04040404040404041%, Difference 17.215386670045177%.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "Significant prediction error: Predicted 16.955562591552734%, Actual 0.0%, Difference 16.955562591552734%.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Significant prediction error: Predicted 16.85093116760254%, Actual 0.0%, Difference 16.85093116760254%.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Significant prediction error: Predicted 16.718595504760742%, Actual 0.494949494949495%, Difference 16.223646009811247%.\n",
      "Allocating 1 VM(s) due to high cumulative usage (84.75141143798828%).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import simpy\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'C:/Users/ratho/OneDrive/Desktop/dataset1.txt'\n",
    "data = pd.read_csv(file_path, header=None)\n",
    "cpu_usage = data.values.flatten()\n",
    "\n",
    "# Sequence length for LSTM\n",
    "sequence_length = 10\n",
    "\n",
    "# Create sequences for LSTM input\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        sequences.append(data[i:i + seq_length])\n",
    "        targets.append(data[i + seq_length])\n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "X, y = create_sequences(cpu_usage, sequence_length)\n",
    "\n",
    "# Reshape data for LSTM (samples, timesteps, features)\n",
    "X_reshaped = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "# Scale the data\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X_reshaped.reshape(-1, sequence_length))\n",
    "X_scaled = X_scaled.reshape(-1, sequence_length, 1)\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, train_size=0.7, shuffle=False)\n",
    "\n",
    "# Define Bidirectional LSTM model\n",
    "model_lstm = Sequential([\n",
    "    Bidirectional(LSTM(50, return_sequences=False), input_shape=(sequence_length, 1)),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_lstm.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model_lstm.fit(X_train, y_train, epochs=10, batch_size=64, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "lstm_predictions = model_lstm.predict(X_test)\n",
    "lstm_predictions_inv = scaler_y.inverse_transform(lstm_predictions)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_lstm = mean_squared_error(y_test, lstm_predictions_inv)\n",
    "rmse_lstm = np.sqrt(mse_lstm)\n",
    "r2_lstm = r2_score(y_test, lstm_predictions_inv)\n",
    "\n",
    "print(f'Bidirectional LSTM Model - MSE: {mse_lstm:.2f}, RMSE: {rmse_lstm:.2f}, R²: {r2_lstm:.2f}')\n",
    "\n",
    "# SimPy Datacenter Simulation\n",
    "class Datacenter:\n",
    "    def __init__(self, env, capacity=10):\n",
    "        self.env = env\n",
    "        self.server = simpy.Resource(env, capacity=capacity)\n",
    "\n",
    "    def process_vm_request(self, vm, duration):\n",
    "        with self.server.request() as req:\n",
    "            yield self.env.timeout(duration)\n",
    "\n",
    "class VMAllocator:\n",
    "    def __init__(self, env, threshold=80):\n",
    "        self.env = env\n",
    "        self.threshold = threshold\n",
    "        self.cumulative_cpu_usage = 0\n",
    "        self.vm_counter = 0\n",
    "\n",
    "    def allocate_vms(self, predicted_value, actual_value):\n",
    "        self.cumulative_cpu_usage += predicted_value\n",
    "        difference = abs(predicted_value - actual_value)\n",
    "\n",
    "        if difference > 10:\n",
    "            print(f\"Significant prediction error: Predicted {predicted_value}%, Actual {actual_value}%, Difference {difference}%.\")\n",
    "        \n",
    "        if self.cumulative_cpu_usage > self.threshold:\n",
    "            num_vms = int((self.cumulative_cpu_usage - self.threshold) // 10) + 1\n",
    "            print(f\"Allocating {num_vms} VM(s) due to high cumulative usage ({self.cumulative_cpu_usage}%).\")\n",
    "            self.cumulative_cpu_usage = 0 \n",
    "\n",
    "env = simpy.Environment()\n",
    "\n",
    "datacenter = Datacenter(env)\n",
    "vm_allocator = VMAllocator(env)\n",
    "\n",
    "# Predict values and simulate VM allocation\n",
    "last_n_values = X_scaled[-1]  \n",
    "predicted_values = []\n",
    "actual_values = y_test[:10]  \n",
    "\n",
    "for i, actual_value in enumerate(actual_values):\n",
    "    # Reshape the last_n_values for LSTM input (1, sequence_length, 1)\n",
    "    last_n_values_reshaped = last_n_values.reshape(1, sequence_length, 1)\n",
    "    \n",
    "    # Predict the next value\n",
    "    predicted_value = scaler_y.inverse_transform(\n",
    "        model_lstm.predict(last_n_values_reshaped).reshape(-1, 1)\n",
    "    ).flatten()[0]\n",
    "    predicted_values.append(predicted_value)\n",
    "\n",
    "    # Allocate VMs based on the predicted and actual values\n",
    "    vm_allocator.allocate_vms(predicted_value, actual_value)\n",
    "\n",
    "    # Update last_n_values for the next prediction\n",
    "    last_n_values = np.append(last_n_values[1:], scaler_y.transform([[predicted_value]])).flatten()\n",
    "\n",
    "    # Reshape last_n_values for the next LSTM prediction\n",
    "    last_n_values_reshaped = last_n_values.reshape(1, sequence_length, 1)\n",
    "\n",
    "    # Process VM request in the datacenter simulation\n",
    "    vm = f\"VM_{i}_Predicted_{predicted_value:.2f}\"\n",
    "    env.process(datacenter.process_vm_request(vm, 10))\n",
    "\n",
    "# Run the simulation\n",
    "env.run(until=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
